{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83ac3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from scipy.signal import detrend\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ece78bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_with_edge_pad(x, shift):\n",
    "    if shift > 0:\n",
    "        pad = np.full(shift, x[0])\n",
    "        return np.concatenate([pad, x[:-shift]])\n",
    "    elif shift < 0:\n",
    "        pad = np.full(-shift, x[-1])\n",
    "        return np.concatenate([x[-shift:], pad])\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f207e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total normal (non-anomaly) segments: 344\n",
      "Original (non-augmented): 172\n",
      "Augmented (shifted): 172\n"
     ]
    }
   ],
   "source": [
    "folder = './Dataset/Open/Edited_plant_split/Plectranthus/train'\n",
    "shift_amount_sec = 1.0\n",
    "segment_len_sec = 3.0\n",
    "\n",
    "normal_segments = []\n",
    "segment_labels = []\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith('.wav'):\n",
    "        wav_path = os.path.join(folder, file)\n",
    "        txt_path = os.path.join(folder, file.replace('.wav', '-events.txt'))\n",
    "        if not os.path.exists(txt_path):\n",
    "            continue\n",
    "\n",
    "        sr, data = wavfile.read(wav_path)\n",
    "        if data.ndim > 1:\n",
    "            data = data[:, 0]\n",
    "        data = detrend(data)\n",
    "        data_norm = (data - np.mean(data)) / np.std(data)\n",
    "        seg_len = int(segment_len_sec * sr)\n",
    "\n",
    "        # --- Parse all (1,2) marker pairs ---\n",
    "        markers = []\n",
    "        with open(txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip() and line[0].isdigit():\n",
    "                    m_id, m_time = line.split(',')\n",
    "                    markers.append((int(m_id.strip()), float(m_time.strip())))\n",
    "\n",
    "        # Make ordered list of anomaly regions (sample indices)\n",
    "        anomaly_regions = []\n",
    "        temp_start = None\n",
    "        for m_id, m_time in markers:\n",
    "            if m_id == 1:\n",
    "                temp_start = int(m_time * sr)\n",
    "            elif m_id == 2 and temp_start is not None:\n",
    "                temp_end = int(m_time * sr)\n",
    "                anomaly_regions.append((temp_start, temp_end))\n",
    "                temp_start = None\n",
    "\n",
    "        # If no valid anomaly region, treat entire file as normal\n",
    "        if not anomaly_regions:\n",
    "            normal_regions = [(0, len(data_norm))]\n",
    "        else:\n",
    "            # Compute the normal (non-anomaly) regions\n",
    "            normal_regions = []\n",
    "            last_end = 0\n",
    "            for start, end in anomaly_regions:\n",
    "                if last_end < start:\n",
    "                    normal_regions.append((last_end, start))\n",
    "                last_end = end\n",
    "            if last_end < len(data_norm):\n",
    "                normal_regions.append((last_end, len(data_norm)))\n",
    "\n",
    "        # --- Extract non-anomaly segments from original signal ---\n",
    "        for n_start, n_end in normal_regions:\n",
    "            # Only full segments\n",
    "            for i in range(n_start, n_end - seg_len + 1, seg_len):\n",
    "                normal_segments.append(data_norm[i:i + seg_len])\n",
    "                segment_labels.append(0)\n",
    "\n",
    "        # --- Augmentation ---\n",
    "        max_shift = int(shift_amount_sec * sr)\n",
    "        shift_amount = random.randint(-max_shift, max_shift)\n",
    "        data_shifted = shift_with_edge_pad(data_norm, shift_amount)\n",
    "\n",
    "        # Shift all region indices\n",
    "        anomaly_regions_shifted = []\n",
    "        for start, end in anomaly_regions:\n",
    "            s = max(0, min(len(data_shifted)-1, start + shift_amount))\n",
    "            e = max(0, min(len(data_shifted)-1, end + shift_amount))\n",
    "            anomaly_regions_shifted.append((s, e))\n",
    "        # Compute normal regions for shifted\n",
    "        if not anomaly_regions_shifted:\n",
    "            normal_regions_shifted = [(0, len(data_shifted))]\n",
    "        else:\n",
    "            normal_regions_shifted = []\n",
    "            last_end = 0\n",
    "            for start, end in anomaly_regions_shifted:\n",
    "                if last_end < start:\n",
    "                    normal_regions_shifted.append((last_end, start))\n",
    "                last_end = end\n",
    "            if last_end < len(data_shifted):\n",
    "                normal_regions_shifted.append((last_end, len(data_shifted)))\n",
    "        # Extract non-anomaly segments from shifted\n",
    "        for n_start, n_end in normal_regions_shifted:\n",
    "            for i in range(n_start, n_end - seg_len + 1, seg_len):\n",
    "                normal_segments.append(data_shifted[i:i + seg_len])\n",
    "                segment_labels.append(1)\n",
    "\n",
    "normal_segments = np.array(normal_segments, dtype=np.float32)\n",
    "segment_labels = np.array(segment_labels, dtype=np.int32)\n",
    "print(\"Total normal (non-anomaly) segments:\", len(normal_segments))\n",
    "print(\"Original (non-augmented):\", np.sum(segment_labels == 0))\n",
    "print(\"Augmented (shifted):\", np.sum(segment_labels == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c19480af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSegDataset(Dataset):\n",
    "    def __init__(self, segments):\n",
    "        self.data = torch.from_numpy(segments)\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.data[idx]\n",
    "\n",
    "dataset = AudioSegDataset(normal_segments)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69972080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a61b6fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded threshold: 0.06286919116973877\n",
      "Saved mean latent embedding to ./Dataset/Open/Model/autoencoder/MAE/Solanum_mean_latent.pt\n"
     ]
    }
   ],
   "source": [
    "n = normal_segments.shape[1]\n",
    "\n",
    "load_path = './Dataset/Open/Model/autoencoder/MAE/Solanum.pth'\n",
    "with open('./Dataset/Open/Model/autoencoder/MAE/Solanum.json', 'r') as f:\n",
    "    threshold = json.load(f)['threshold']\n",
    "print(f\"Loaded threshold: {threshold}\")\n",
    "\n",
    "model = Autoencoder(n)\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.eval()\n",
    "\n",
    "splits = [\n",
    "    ('train', './Dataset/Open/Edited_plant_split/Solanum/train'),\n",
    "    ('test', './Dataset/Open/Edited_plant_split/Solanum/test')\n",
    "]\n",
    "\n",
    "split_ious = {'train': [], 'test': []}\n",
    "split_success = {'train': [], 'test': []}\n",
    "\n",
    "success_threshold = 0.5   # Set IoU threshold for successful detection\n",
    "\n",
    "# ---- Extract mean latent embedding from normal (non-anomaly) segments ----\n",
    "\n",
    "latent_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for split_name, folder in splits:\n",
    "        wav_files = [file for file in os.listdir(folder) if file.endswith('.wav')]\n",
    "        for wav_file in wav_files:\n",
    "            txt_file = wav_file.replace('.wav', '-events.txt')\n",
    "            txt_path = os.path.join(folder, txt_file)\n",
    "            wav_path = os.path.join(folder, wav_file)\n",
    "            if not os.path.exists(txt_path):\n",
    "                continue\n",
    "\n",
    "            # Load and preprocess\n",
    "            sr, data = wavfile.read(wav_path)\n",
    "            if data.ndim > 1:\n",
    "                data = data[:, 0]\n",
    "            data = detrend(data)\n",
    "            data_norm = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "            # Load anomaly markers\n",
    "            with open(txt_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            markers = {}\n",
    "            for line in lines:\n",
    "                if line.strip() and line[0].isdigit():\n",
    "                    m_id, m_time = line.split(',')\n",
    "                    markers[int(m_id.strip())] = float(m_time.strip())\n",
    "            seg_len = int(segment_len_sec * sr)\n",
    "\n",
    "            # Find anomaly regions\n",
    "            anomaly_regions = []\n",
    "            temp_start = None\n",
    "            for line in lines:\n",
    "                if line.strip() and line[0].isdigit():\n",
    "                    m_id, m_time = line.split(',')\n",
    "                    m_id = int(m_id.strip())\n",
    "                    m_time = float(m_time.strip())\n",
    "                    if m_id == 1:\n",
    "                        temp_start = int(m_time * sr)\n",
    "                    elif m_id == 2 and temp_start is not None:\n",
    "                        temp_end = int(m_time * sr)\n",
    "                        anomaly_regions.append((temp_start, temp_end))\n",
    "                        temp_start = None\n",
    "            # Find normal (non-anomaly) regions\n",
    "            if not anomaly_regions:\n",
    "                normal_regions = [(0, len(data_norm))]\n",
    "            else:\n",
    "                normal_regions = []\n",
    "                last_end = 0\n",
    "                for start, end in anomaly_regions:\n",
    "                    if last_end < start:\n",
    "                        normal_regions.append((last_end, start))\n",
    "                    last_end = end\n",
    "                if last_end < len(data_norm):\n",
    "                    normal_regions.append((last_end, len(data_norm)))\n",
    "            # Extract normal segments and get latents\n",
    "            for n_start, n_end in normal_regions:\n",
    "                for i in range(n_start, n_end - seg_len + 1, seg_len):\n",
    "                    segment = data_norm[i:i + seg_len]\n",
    "                    segment_tensor = torch.from_numpy(segment).float().unsqueeze(0)\n",
    "                    latent = model.encoder(segment_tensor)  # shape [1, latent_dim]\n",
    "                    latent_list.append(latent.squeeze(0))   # shape [latent_dim]\n",
    "\n",
    "# Stack all latent vectors and compute the mean\n",
    "if len(latent_list) > 0:\n",
    "    latent_all = torch.stack(latent_list)    # [N, latent_dim]\n",
    "    mean_latent = latent_all.mean(dim=0)     # [latent_dim]\n",
    "    # Save mean latent embedding\n",
    "    mean_latent_path = load_path.replace('.pth', '_mean_latent.pt')\n",
    "    torch.save(mean_latent, mean_latent_path)\n",
    "    print(f\"Saved mean latent embedding to {mean_latent_path}\")\n",
    "else:\n",
    "    print(\"No normal segments found to extract mean latent embedding.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
